# RNN

Q1：***RNN和CNN之间有什么区别？在哪些情况下分别使用彼此？***

- CNN对数据执行卷积操作，提取其中的特征；CNN最适用于需要位置不变性的情况（无论目标在输入样本中的什么位置，您都希望捕获相似的特征）
- RNN是在输入样本之间保持状态的神经网络，RNN能利用以前的输入样本信息并使用这些信息来帮助当前输入样本的分类；RNN更适合处理时序数据

Q2：***RNN网络在NLP领域有什么运用？***

- RNN在NLP领域主要有如下应用：序列标注，比如词性标注、命名实体识别，常用blstm-crf模型；分类任务，比如情感分析；关系判断，比如推理、阅读理解等；生成式任务，比如翻译、对话、文本摘要等通过一段文字生成另一段文字的任务

Q3：***为什么在处理时间序列问题上，RNN会优于MLP？***

- MLP在处理时间序列时，不考虑输入之间的时间关系，$t-1，t，t+1$时刻的数据被同等看待，无法利用到上下文信息；
- 而在 RNN 中，先前时间步状态的输出作为当前时间步状态的输入传递，RNN能更好地利用到时间序列中的历史信息。

Q4：***RNN的输入有几个维度？每个维度分别代表什么？RNN的输出呢？***

- RNN的输入有三个维度，每个维度分别表示batch_size、seq_len、input_dim，batch_size表示batch大小，seq_len表示序列的长度，input_dim表示输入数据的维度
- 输出层同样拥有三个维度，含义与输入基本一致，input_dim修改为output_dim

Q5：***RNN和传统的全连接网络之间有什么差别？***

- MLP是最简单的DNN，它的每一层其实就是fc层（fully connected layer）
- RNN相对于MLP而言，多了时间先验知识，即RNN可以学习到数据之间的时间相关性。比如一段文字，前面的字是“上”，后面的字是“学”概率更大，是“狗”的概率很小。RNN神经元的输入会有多个time step，每个time step的输入进入神经元中时会共享参数，RNN可以视作为编码了时间相关性的DNN

Q6：***训练RNN时会碰到哪些困难？怎么解决？***

- RNN对于距离当前时刻比较远的隐藏层，容易出现梯度爆炸或者梯度弥散；这是由于在反向传播求解梯度过程中，各个隐藏层的参数会累乘，当特征值小于1时，不断相乘导致特征值向0衰减。特征值大于1时，则向正无穷扩增；导致无法利用梯度来对距离比较远的时刻进行调节
- 解决方法是LSTM网络，它通过门控（gates）的方式，使得不断相乘的梯度的积保持在接近1的数值，从而使得长序列仍然可以训练

Q7：***在机器翻译领域，为什么encoder-decoder结构的RNN取代了seq2seq RNN？***

- seq2seq RNN一次只能翻译一个词汇，而encoder-decoder RNN能处理变长的输入/输出，一次网络推理能翻译一整句话

引用：[https://quizlet.com/284587332/deep-learning-midterm-2-flash-cards/](https://quizlet.com/284587332/deep-learning-midterm-2-flash-cards/)

Q8：***Stateful RNN与Stateless RNN有什么区别？两种RNN结构有什么利弊？***

- stateless RNN每次接收和处理新批次时，RNN的初始状态都会重置为零，因此不会利用已经学习的内部激活（状态），这会导致模型忘记从前几批中学到的知识；
- Stateful RNN每个批次的 LSTM 单元和隐藏状态都使用前一批次的学习状态进行初始化，从而使模型学习跨批次的依赖关系
- 如果训练语料是用相互独立的句子组成，可以使用stateless RNN；如果语料之间具有联系，使用stateful RNN；随着 batch size 的增加，Stateless LSTM 倾向于模拟 Stateful LSTM

引用：[https://towardsai.net/p/l/stateless-vs-stateful-lstms](https://towardsai.net/p/l/stateless-vs-stateful-lstms)

Q9：***分别在什么时候选用MLP、CNN、RNN？***

- MLP 学习从输入到输出的映射关系，适用于为输入分配类别或标签的分类预测问题，也适用于回归预测问题
- CNN将图像数据映射为抽象特征，CNN适合处理具有空间关系的数据，在图像任务和文档分类任务上都取得较好成绩
- RNN的优势是处理序列预测问题，RNN一般用于处理单词和段落序列等时间预测问题上

引用：[https://machinelearningmastery.com/when-to-use-mlp-cnn-and-rnn-neural-networks/](https://machinelearningmastery.com/when-to-use-mlp-cnn-and-rnn-neural-networks/)

Q10：***解释RNN中梯度消失的原因？***

- 在时间维度上RNN等同于一个深层的MLP网络，RNN的梯度需要沿着时间步反传，在某个时刻对参数的梯度，需要追溯这个时刻之前所有时刻的信息；时间序列越长，那么其反馈的梯度信号越不显著，最后可能完全没有起作用，这就意味着RNN对长距离语义的捕捉能力失效

Q11：***讲讲LSTM的基础结构，相对RNN做了哪些改进？***

- LSTM引入了门控机制，包括遗忘门、输入门、输出门，输入门决定何时让输入进入细胞单元；遗忘门决定何时应该记住前一时刻的信息；输出门决定何时让记忆流入下一时刻；相比RNN只有一个传递状态，LSTM有两个传输状态$c_{t}$和$h_{t}$，$c_{t}$的梯度会无损地传递到$c_{t-1}$，缓解长距离梯度消失的问题
- LSTM通过门控机制将短期记忆和长期记忆结合起来，一定程度上解决了梯度消失的问题。

Q12：***LSTM和GRU结构上的区别有哪些？***

- 相比LSTM，GRU只有两个门（重置门和更新门），重置门决定了如何将新的输入信息与前面的记忆相结合，更新门定义了前面记忆保存到当前时间步的量
- GRU不保留内部记忆$c_{t}$，GRU直接将hidden state传给下一单元，LSTM通过记忆单元将hidden state包装起来。

Q13：*****还有哪些其它的解决梯度消失或梯度爆炸的方法？*****

- 梯度裁剪gradient clipping，当BP时的梯度小于某个阈值或大于某个阈值时 ，直接裁剪，防止太小的梯度累乘带来的梯度消失或太大的梯度累乘带来的梯度爆炸。
- 改变激活函数，例如减少使用sigmoid、tanh这类激活函数，改成使用Relu、LeakRelu等
- 残差结构，类似于CEC的模块，跨层的连接结构能让梯度无损的进行后向传播。
- Batch Normalization，相当于对每一层的输入做了一个规范化，强行把这个输入拉回标准正态分布*N~(0,1)。*这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数的大变化，进而梯度变大，避免产生梯度消失问题。而且梯度变化大 意味着学习收敛速度快，加快模型的训练速度。

Q14：***LSTM模型中存在sigmoid和tanh函数，两者分别用于哪里？***

- sigmoid函数被用作各个门上，产生0~1之间的值，来判断是对信息进行记住还是遗忘。
- tanh用在了状态和输出上，可以替换为其他激活函数

Q15：***为什么LSTM具有长时记忆能力？***

- 因为遗忘门控制上一时间步的记忆细胞 $C_{t-1}$是否传递到当前时间步，如果遗忘门的输出一直近似1且输入门一直近似0，过去的记忆细胞$C_{t-1}$将一直通过时间保持到当前时间步$C_{t}$，这样可以捕捉到时间序列中时间步跨度较大的依赖关系，也可以应对RNN的梯度衰减

引用：https://www.zhihu.com/question/317594964/answer/641226608

Q16：**画出LSTM的结构**

![Untitled](RNN%20ba2dd072fd1a45eab8be29543b3c4178/Untitled.png)