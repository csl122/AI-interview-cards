# Deep Learning Fundamental

# 初级

Q1：***深度学习和机器学习本质的区别在哪里？***

- 机器学习依赖于人类的因素更多，人需要手动选择更有效的特征并进行组合；深度学习使得大部分特征提取的步骤自动化，消除了人为干预的因素
- 机器学习对数据量的依赖程度低，**深度学习**需要大量数据才能提供更高精度

Q2：***深度学习的性能为什么会随着数据增多而提高？***

- 当输入学习算法的数据数量增加时，模型将考虑更多的边缘情况，因此算法将学习在这些边缘情况下做出正确的决策，提供更好的泛化性

Q3：***什么是ensemble learning，在深度学习中有何运用？***

- ensemble learning表示使用两个以上的模型学习，并使用某种规则将各个模型的学习结果整合，用于提高整体模型的*泛化*能力
- 深度学习中ensemble learning的运用有dropout、zoneout和drop-connect

Q4：**深度学习中，如何选择激活函数**？

- 如果要预测的输出是二元类的概率，那么可以使用Sigmoid 函数做网络输出层。
- 如果要预测的输出有两个类别，则可以使用Tanh 函数做网络输出层。
- 由于ReLU 函数计算简单，有利于抑制梯度消失情况，在网络中间层普遍使用

![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled.png)

Q5：***相比机器学习，深度学习有什么优势？***

- 深度学习不需要设计模型的人对特征工程有很多理解，深度学习是端到端的，有利于快速上手
- 深度学习在数据集较大、建模问题较复杂的情况下，准确率更高

Q7：***如何判断深度学习模型中存在梯度消失问题？***

- 模型loss在训练阶段下降的非常缓慢
- 训练停止后（达到一定epoch），模型loss仍然很大
- 模型权重呈指数级缩小，训练模型阶段就变得非常小
- 训练阶段，模型权重变为零值

Q8：***如何判断深度学习模型是否存在梯度爆炸问题？怎么确定？***

- 以下现象表明你训练的模型可能存在梯度爆炸的问题，例如：模型无法很好地学习训练数据（模型损失一直很高）、模型训练不稳定（每一次更新，loss变化很大）、训练时loss变为NaN；
- 当发生以上现象时，可以深入挖掘模型看看是否有梯度爆炸的问题，以下现象可以确认发生梯度爆炸：训练阶段模型权重变为NaN；在训练期间，每个节点和层的误差梯度值始终高于`1.0`；

Q9：***Embedding Layer的作用是什么？***

- embedding层的目的是将模型的输入，one-hot向量（一种高维、稀疏的表示）映射到低维稠密的表示，节省模型的计算量

Q10：***什么是早停策略？***

- early-stopping：正则化的一种形式。NN使用梯度下降更新网络参数，但过了某个训练阶段，改进模型对训练数据的拟合会导致泛化误差增加。过了那个点，改进模型对训练数据的拟合会导致泛化误差增加。early-stopping规定了网络可以训练的迭代次数。

Q11：***什么是增量学习？***

- 增量学习是指算法从先前可用的数据集中生成分类器后，再从新数据中学习的能力
- 增量学习算法不断使用输入数据来扩展现有模型的知识，即使用动态的数据流，进一步训练模型；目的是让学习模型在不忘记其现有知识的情况下适应新数据

引用：[https://link.springer.com/](https://link.springer.com/)

Q12：***解释感知机的原理***

- 感知机是一种用于二元分类器监督学习的算法。其输入为实例的特征向量，输出为实例的类别，+1代表正类，-1代表负类。感知机属于判别模型，它的目标是要将输入实例通过分离超平面将正负二类分离。
- 感知机学习的目标就是求能将正负样本完全分开的分离超平面,即要寻找超平面参数w，b；感知机通过监督学习学习参数。

![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%201.png)

# 中级

Q13：***什么是计算图？哪些算法属于计算图模型？***

- 计算图是具有等式数据的图。它们是表示数学表达式的有向图的一种形式。一个很常见的例子是后缀、中缀和前缀的计算。图中的每个节点都可以包含操作、变量或方程本身。这些图表出现在计算机中进行的大多数计算中。

![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%202.png)

- 感知机、深层神经网络、线性回归均属于计算图模型

引用：[https://towardsdatascience.com/evolution-of-graph-computation-and-machine-learning-3211e8682c83](https://towardsdatascience.com/evolution-of-graph-computation-and-machine-learning-3211e8682c83)

Q14：***线性激活函数和非线性激活函数之间有什么区别？***

- 线性激活函数将输出预测为输入的线性函数，非线性函数（如tanh、sigmoid、relu）的输出是输入的非线性函数；非线性激活函数可以使神经网络学习更复杂的行为模式

Q15：***解释1*1卷积操作的意义？***

- 1x1卷积操作，可以改变数据在通道上的维度，$（c_{in},H,W）$为数据输入，经过$(c_{out},1,1)$的卷积核处理后，输出数据为$(c_{out},H,W)$
- 在 Inception 架构中，我们使用 1x1 卷积滤波器来降低滤波器维度中的维度

Q16：***在设计神经网络时，神经网络的广度和深度哪个更重要？***

- 对于标准的前馈神经网络，深度比宽度更有价值；深度可以为网络增加更多的非线性，增强网络的学习能力；但同时，越深的模型越容易过拟合。
- （相关研究论文：[http://proceedings.mlr.press/v49/eldan16.pdf](http://proceedings.mlr.press/v49/eldan16.pdf)）。

引用：[https://www.quora.com/What-are-the-tradeoffs-of-increasing-the-depth-of-a-neural-network](https://www.quora.com/What-are-the-tradeoffs-of-increasing-the-depth-of-a-neural-network)

Q17：***神经网络中的隐藏层计算的内容是什么？***

- 隐藏层提供非线性变换；提取输入中的高维特征
- 隐藏层将输入转换为输出可用的特征，并将输入转换为输出需要的维度

引用：[https://stats.stackexchange.com/](https://stats.stackexchange.com/)

Q18：***神经网络中，可以将非线性激活函数替换为线性激活函数吗？为什么？***

- 不可以，非线性激活函数在神经网络中是不可或缺的；这种非线性是神经网络和简单线性回归模型的重要区别，它**允许神经网络学习复杂的函数表示**

Q19：***在深度学习模型中，如何选择损失函数？***

- 损失函数的选择必须与特定预测建模问题的框架相匹配，例如分类或回归问题使用不同的损失函数；此外，输出层的配置也必须适合所选的损失函数。
- 回归损失函数：使用均方误差损失、均方对数误差损失、平均绝对误差损失
- 二元分类损失函数：交叉熵、Hinge Loss；
- 多元分类损失函数：多类交叉熵损失、K-L散度；K-L散度通过计算信息量的丢失衡量loss，当使用学习近似更复杂函数的模型而不是简单的多类分类时，KL 散度损失函数更常用，例如在用于学习密集特征表示的自动编码器的情况；

引用：[https://machinelearningmastery.com/](https://machinelearningmastery.com/)

Q20：***介绍GAN的原理***

- GAN，对抗生成网络，是生成模型的一种，模型训练是在对抗博弈状态中找到平衡
- GAN的主要结构包括一个生成器G（Generator）和一个判别器D（Discriminator）。生成网络从潜在空间（latent space）中随机采样作为输入，其输出结果需要尽量模仿训练集中的真实样本。判别网络的输入则为真实样本或生成网络的输出，其目的是将生成网络的输出从真实样本中尽可能分辨出来。而生成网络则要尽可能地欺骗判别网络。两个网络相互对抗、不断调整参数，最终目的是使判别网络无法判断生成网络的输出结果是否真实。

![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%203.png)

Q21：***什么是迁移学习？***

- 迁移学习(transfer learning)通俗来讲，就是运用已有的知识来学习新的知识，核心是找到已有知识和新知识之间的相似性
- 迁移学习是机器学习中早有的概念，按照学习方式可以分为基于样本的迁移，基于特征的迁移，基于模型的迁移，以及基于关系的迁移。
- 基于样本的迁移通过对源域中有标定样本的加权利用完成知识迁移；基于特征的迁移通过将源域和目标域映射到相同的空间（或者将其中之一映射到另一个的空间中）并最小化源域和目标域的距离来完成知识迁移；基于模型的迁移将源域和目标域的模型与样本结合起来调整模型的参数；基于关系的迁移则通过在源域中学习概念之间的关系，然后将其类比到目标域中，完成知识的迁移。

![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%204.png)

Q22：***卷积层什么时候与全连接层等价？***

- 实际上，全连接层是卷积层的一个子集；如果把kernel大小设置为与输入图像的大小一致，设置通道数与全连接层层中的输出神经元大小一致，那么卷积层等价于全连接层

Q23：***GAN和自动编码器有什么区别和联系？***

- 自动编码器的工作是同时学习编码网络和解码网络，将输入（例如图像）提供给编码器，编码器试图将输入降维为压缩的编码形式，然后将其馈送到解码器。自编码器的学习目标是reconstruction los，通常用于降维、异常检测等领域
- 生成对抗网络的工作是通过博弈的过程学习生成器和鉴别器，生成器生成以假乱真的图片，鉴别器检测图像是否是真实的；GAN通常用于图像生成等领域
- GAN与AE同属于生成模型；但GAN与AE的学习目标不同，运用领域也不同

引用：[https://stackoverflow.com](https://stackoverflow.com/)

Q24：***预训练对神经网络有什么好处？***

- 预训练模型能够从海量未标注的数据上学习语言本身的知识，而后在少量带标签的数据上微调，从而使下游任务能够更好地学习到语言本身的特征和特定任务的知识。预训练模型不仅能够充分利用广泛的网络资源，而且还能完美地解决人工标记数据较为复杂的问题

Q25：***在深度神经网络中使用Batch-Norm会有什么问题吗？***

- Batchnorm对一个batch的数据进行归一化，可获得更快的收敛速度和更高的准确性；但Batchnorm也有它的劣势：
    - Batchnorm是内存密集任务，所有batch处理的统计信息需要存储在该层中，消耗大量计算成本
    - 训练和推理之间存在差异，推理结果取决于batch大小
    - 打破了小批量训练示例之间的独立性
    - 模型性能对batch大小敏感，batch过小时，batch-norm性能不佳
    - batchnorm不适合序列问题，**在序列模型中，我们可能拥有长度不同的序列，以及对应于较长序列的较小批量大小**
- 针对以上缺点，目前的改善方案有layer-norm、instance-norm等

引用：[https://medium.com/geekculture](https://medium.com/geekculture)

Q26：***什么是multitask learning？什么时候使用它较为合适？***

- multitak-learning训练模型以同时执行多个任务；在深度学习中，MTL 是指通过跨任务共享网络的某些层和参数来训练神经网络执行多项任务。MTL通过利用跨任务共享的信息来提高模型的泛化性能。通过共享网络的一些参数，模型可以学习更高效、更紧凑的数据表示
- 当学习任务之间具有一定的相关性时，使用多任务学习有帮助。或者，当任务之间共享基本原则或信息时，多任务学习可以提高性能

引用：[https://towardsdatascience.com/](https://towardsdatascience.com/)

Q27：***AE与VAE的区别是什么？***

- AE解码器的输入是编码器的输出，而VAE解码器的输入是从具有编码器输出的均值和方差的高斯分布中采样的；VAE具有生成能力。
- VAE 解决了 AE 的非正则化潜在空间问题，这使得它能够从来自潜在空间的随机采样向量生成数据

![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%205.png)

引用：[https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2](https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2)

Q28：***说一些你知道的CNN架构***

- 典型的 CNN 架构堆叠了几个卷积层，然后是一个池化层，然后是另外几个卷积层（+ReLU），然后是另一个池化层，依此类推
- 典型的CNN架构如：LetNet、AlexNet、GoogleNet、Inception V2 V3 V4、VGG、ResNet、DenseNet、ResNeXt、Channel Boosted CNN、EfficientNet

引用：[https://www.kaggle.com/general/255114](https://www.kaggle.com/general/255114)

Q29：***什么是玻尔兹曼机？***

- 玻尔兹曼机是是由对称连接的节点组成的神经网络，其中节点做出二元决策，自行决定是否激活。玻尔兹曼机可以串在一起形成更复杂的系统，例如深度信念网络；
- 玻尔兹曼机通常用于解决不同的计算问题，例如，对于搜索问题，连接上的权重可以固定，并用于表示优化问题的成本函数

![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%206.png)

引用：[https://deepai.org/](https://deepai.org/)

Q30：***你会选择那种神经网络完成视频分类任务？***

- 视频由有序的帧序列组成。每个帧都包含*空间* 信息，而这些帧的序列包含*时间*信息。为了对这两个方面进行建模，可以使用由卷积（用于空间处理）和循环层（用于时间处理）组成的混合架构CRNN来进行分类

Q31：***dropout对神经网络训练有什么影响？***

- dropout通过随机使一些神经网络节点失活，可以使用单个模型来模拟具有大量不同网络架构的模型；它提供了一种计算成本非常低且非常有效的正则化方法，以减少过度拟合并改善各种深度神经网络中的泛化误差。

Q32：***AE模型可以用于特征生成吗？如果可以，怎么做？***

- 可以；AE模型可以用于学习原始数据的压缩表示，该表示可作为特征；
- 做法：训练AE重建输入数据，在模型训练完成后，丢弃decoder，使用模型在bottle-neck处的输出作为输入数据的特征，该特征保留了输入数据中的大部分信息，并且是固定维度的

Q33：***Batch Normalization、Instance Normalization、Layer Normalization、Group Normalization之间有什么区别？***

- 四种归一化方式如下图，立方体的3个维度为别为batch/ channel/ H*W。BN计算均值和标准差时，固定channel(在一个channel内)，对HW和batch作平均；LN计算均值和标准差时，固定batch(在一个batch内)，对HW和channel作平均；IN计算均值和标准差时，同时固定channel和batch(在一个batch内中的一个channel内)，对HW作平均；GN计算均值和标准差时，固定batch且对channel作分组(在一个batch内对channel作分组)，在分组内对HW作平均

引用：《Group Normalization》

![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%207.png)

Q34：***同等参数量情况下，深层神经网络比浅层神经网络效果好的原理是什么？***

- 隐藏层的操作，可以被看作在输入向量的线性组合上施加非线性激活函数，每个隐藏层都可以看作是一个模块化函数。许多隐藏层的深层网络就像是多个函数的堆叠，与浅层网络相比，相同数量的参数可以实现更复杂的功能。换句话说，深度网络更有效地利用参数
- 以具体任务举例，如语音识别，初始层学习边缘和曲线等低级特征（发音方式），后面的层学习高级特征（音素）；具有许多隐藏层的深层网络比浅层网络更容易学习这种高级特征

Q35：***比较svm和deep learning***

- 直观区别：deep learning是将许多神经网络层堆叠起来，以分层的方式学习越来越多的数据的抽象表示；而SVM是在数据点中找到一个分裂边界（线性可分情况下的超平面），该边界与任一类别的支持向量尽可能远；
- deep learning相对来说不可被解释，而SVM有强有力的数学推导支撑；
- 对权重初始化的敏感性：因为神经网络使用梯度下降，这使得它们对其权重矩阵的初始随机化敏感。这是因为，如果初始随机化使神经网络接近优化函数的局部最小值，则准确度将永远不会增加超过某个阈值。相反，SVM 更可靠，无论其初始配置如何，它们都能保证收敛到全局最小值。
- 在多类别问题上，deep learning更方便；SVM需要使用多个SVM分类器来处理多分类问题，而deep learning只需要单个神经网络。
- 在特征数大于样本数时，SVM表现不佳；SVM需要在特征工程上做更多的工作；

Q36：***Relu激活函数比sigmoid函数好的地方在哪里？***

- Relu激活函数计算量更小；
- Relu能解决sigmoid激活函数容易引起梯度消失的问题；从两者梯度图可以看出，Sigmoid和tanh函数反向传播的过程中，饱和区域非常平缓，接近于0，容易出现梯度消失的问题，减缓收敛速度。Relu的gradient大多数情况下是常数，有助于解决深层网络的收敛问题
- 由于Relu不容易梯度消失，导致网络训练更快
- ReLU会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生
    
    ![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%208.png)
    

![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%209.png)

Q37：***傅里叶变换在深度学习中有什么运用？***

- 通过使用傅里叶变换，可以节省卷积层的计算量；CNN中输入和滤波器的矩阵可以转换到频域进行乘法，频域乘法的结果矩阵可以转换到时域。矩阵从时域到频域的转换可以通过傅里叶变换或快速傅里叶变换来完成，从频域到时域的转换可以通过傅里叶逆变换或快速傅里叶逆变换来完成。

![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%2010.png)

Q38：***集成学习在深度学习中有什么运用？***

- 集成学习是指训练多个模型，并结合这些模型的预测，减少最终预测的方差，并提高准确性；集成学习可以从数据、模型、结果组合这三个方面应用于深度学习
- 改变训练数据：k 折交叉验证（k 个不同的模型在 k 个不同的训练数据子集上进行训练。将这 k 个模型保存并用作整体的成员）、重采样（对训练数据集进行重采样，然后使用重采样的数据集训练网络）
- 模型：dropout、drop-connect
- 结果组合：加权平均集成（对每个模型的预测进行加权来稍微改善，使用保留验证数据集优化权重）、模型权重平均（对多个网络的权重进行平均，以希望产生一个整体性能优于任何原始模型的新单一模型）

引用：[https://machinelearningmastery.com/](https://machinelearningmastery.com/)

Q39：***深度学习怎么减轻/避免维度诅咒？***

- 维数灾难通常是因为数据中存在相关和太多不相关的（噪声）特征。深度学习架构可以提取高维数据表示的低维特征，因此 DL 系统在学习过程中减少了不相关特征的影响，同时增加了相关特征的影响。

引用：[https://www.quora.com/](https://www.quora.com/)

Q40：***你了解多少种跳转链接（skip-connection）方法？***

- ****ResNet：****来自初始层的信息通过矩阵加法传递到更深层，此操作没有任何附加参数
- ****DenseNets：****DenseNets 将层的输出特征图与下一层连接起来，而不是求和

![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%2011.png)

- ****U-Net：长跳跃连接****

引用：[https://www.analyticsvidhya.com/](https://www.analyticsvidhya.com/)

Q41：什么是***Deep Recurrent Q-Network*?**

- DRQN是将DQN中第一个卷积后全连接层替换为LSTM的网络；DRQN虽然在每个时间步只能看到一个帧，但成功地整合了时间信息并复制了 DQN 在标准 Atari 游戏和部分观察到的具有闪烁游戏屏幕的等价物上的性能

引用：[https://arxiv.org/abs/1507.06527](https://arxiv.org/abs/1507.06527)

Q42：*****基于区域的目标检测神经网络*(R-CNN)、*Fast R-CNN*和*Faster R-CNN*之间有什么区别？**

- R-CNN使用选择性搜索算法从图像中仅提取 2000 个区域作为区域建议，避免了在图像中选择大量区域引发的计算爆炸；但即使这样，RCNN也不是实时的
- Fast R-CNN不是将区域建议提供给 CNN，而是将输入图像提供给 CNN 以生成卷积特征图。从卷积特征图中，搜索出建议区域并将它们扭曲成正方形，然后通过使用 RoI 池化层，我们将它们重新整形为固定大小，以便可以将其馈送到全连接层。
- Faster R-CNN不是在特征图上使用选择性搜索算法来识别区域建议，而是使用一个单独的网络来预测区域建议。然后使用 RoI 池化层对预测的区域建议进行重塑，然后使用该层对建议区域内的图像进行分类并预测边界框的偏移值。

Q43：***在处理序列问题上，比较HMM模型和RNN模型***

- HMM更简单，并基于强假设：状态转换只依赖于当前状态，而不依赖于过去的任何事情；HMM适合小数据集的情况
- RNN在大数据集上效果更优秀，不需要额外的假设；
- HMM属于生成模型，RNN属于判别模型

引用：[https://stats.stackexchange.com/](https://stats.stackexchange.com/)

Q44：***知道哪些关于目标检测的网络结构？***

- 常见的目标检测网络结构有RetinaNet、YOLO、CenterNet、SSD（单发多框检测器）、区域建议（R-CNN、Fast-RCNN、Faster RCNN、Cascade R-CNN）；大致可以分为两类：单级目标检测与多级目标检测
- YOLO属于单级检测，使用经过端到端训练的单个神经网络将照片作为输入并直接预测每个边界框的边界框和类标签
- 多级检测将目标检测任务分为两个阶段：提取 RoI（感兴趣区域），然后对 RoI 进行分类和回归。面向 2 阶段的对象检测架构示例包括 R-CNN、Fast-RCNN、Faster-RCNN、Mask-RCNN等

引用：[https://www.v7labs.com/](https://www.v7labs.com/blog/object-detection-guide)

![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%2012.png)

Q45：***GAN网络在训练时的难点是什么？***

- GAN 模型存在以下主要问题：
    - 不收敛：模型参数振荡、不稳定且永不收敛
    - 模式崩溃：生成器崩溃，产生有限种类的样本
    - 梯度消失：判别器太成功以至于生成器梯度消失并且什么也学不到
- 模式崩溃是 GAN 中最难解决的问题之一；在理想情况下，生成模型应该能够生成10个数字（对Minst数据集），如果只能生成其中的几个，而错失其它的模式，则为模式崩溃（生成的样本大量重复类似）
- 模式崩溃的原因：目前的深度神经网络只能够逼近连续映射，而输入数据到输出数据的传输映射是具有间断点的非连续映射。换言之，GAN训练过程中，目标映射不在NN的可表示泛函空间之中，这一矛盾导致了收敛困难；如果目标概率测度的支集具有多个联通分支，GAN训练得到的又是连续映射，则有可能连续映射的值域**集中在某一个连通分支**上，即为模式崩溃
- 解决方法：
    - 经验方法：
        - **unrolled GAN（生成器更新参数时不仅考虑当下状态，而且额外考虑K步判别器的反应，避免了短视行为）**
        - **DRAGAN（避免GAN进入坏的局部纳什均衡点）**
    - 理论方法：**根据Brenier理论，传输映射在奇异点处间断；计算Brenier势能函数，判定奇异点集，可避免模式崩溃**
    
    ![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%2013.png)
    

引用：[https://jonathan-hui.medium.com/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b](https://jonathan-hui.medium.com/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)

《Mode Collapse and Regularity of Optimal Transportation Maps》

Q46：***假设你想训练一个分类器，你有大量未标记的训练数据，但只有几千个标记实例。你将如何进行？***

- 使用半监督方法，使用几千个标记示例训练模型A，对未标注的训练数据进行标注，并按置信度从中挑选可信度高的样本，作为标记数据finetune模型A得到结果模型；

Q47：***怎样使用遗传算法优化神经网络？***

- 遗传算法优化BP神经网络分为BP神经网络结构确定、遗传算法优化和BP神经网络预测3个部分。其中，BP神经网络结构确定部分根据拟合函数输入输出参数个数确定，进而确定遗传算法个体的长度。遗传算法优化使用遗传算法来优化BP神经网络的权值和阈值，种群中的每个个体都包含了一个网络所有权值和阈值，个体通过适应度函数计算个体适应度值，遗传算法通过选择、交叉和变异操作找到最优适应度值对应个体。BP神经网络预测用遗传算法得到最优个体对网络初试权值和阈值赋值，网络经训练后预测函数输出。

![Untitled](Deep%20Learning%20Fundamental%204447b34ffdc24815b8f904824306c678/Untitled%2014.png)

引用：[https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0243030](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0243030)
