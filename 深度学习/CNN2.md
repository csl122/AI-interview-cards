# CNN

Q1：***CNN和RNN有什么区别？什么时候使用它们？***

- CNN最适用于需要平移不变性的情况。平移不变性是指，无论目标出现在图像中的哪个位置，它都会检测到同样的这些特征，输出同样的响应。
- RNN是可以记住历史输入状态信息的神经网络，他们记住历史的输入样本，并使用这些样本来帮助对当前输入样本进行分类。当数据顺序很重要时，适合用RNN。因此，在语音、视频（帧是有序的）以及文本处理中，常常用到RNN。一般来说，与时间序列数据（带有时间戳的数据）相关的问题很适合用RNN解决。

Q2：***CNN怎样被用到时间序列预测中？***

- **CNN**从原始输入数据中学习和自动提取特征的能力可以应用于时间序列预测问题。可以将一系列观察结果视为一维图像，CNN 模型可以读取该图像并提取其中特征，创建时间序列的信息表示
- 常见的将cnn用于时间序列预测的例子包括TCN、wavenet；它们是高度抗噪的模型，并且能够提取信息丰富的深度特征

Q3：***在CNN中，Max Pooling与Average Pooling的优缺点是什么？***

- 卷积层参数误差造成估计均值的偏移，max pooling能减小这种误差；邻域大小受限造成的估计值方差增大，average能减小这种误差。也就是说，average对背景保留更好，max对纹理提取更好。对数字识别等任务，一般使用max-pooling。

引用：[https://www.zhihu.com/question/34898241](https://www.zhihu.com/question/34898241)

Q4：***比较CNN和多层感知机MLP***

- MLP由全连接层构成，每个神经元都和上一层中的**所有**节点连接，存在参数冗余；相比之下，CNN由于权重共享，参数更少，方便网络的训练与设计深层网络；
- MLP只接受向量输入，**会丢失像素间的空间信息；CNN接受矩阵和向量输入，能利用像素间的空间关系**
- MLP是CNN的一个特例，**当CNN卷积核大小与输入大小相同时其计算过程等价于MLP**

引用：[https://medium.com/data-science-bootcamp/multilayer-perceptron-mlp-vs-convolutional-neural-network-in-deep-learning-c890f487a8f1](https://medium.com/data-science-bootcamp/multilayer-perceptron-mlp-vs-convolutional-neural-network-in-deep-learning-c890f487a8f1)

*Q5*： *****CNN*中的*全连接层*有什么作用？**  

- 全连接层在整个网络卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数等操作是将原始数据映射到隐层特征空间的话（特征提取+选择的过程），全连接层则起到将学到的特征表示映射到样本的标记空间的作用。换句话说，就是把特征整合到一起（高度提纯特征），方便交给最后的分类器或者回归。



*Q6*：*****解释RELU激活函数在卷积神经网络中的意义*****

- 在每次卷积操作之后，使用RELU操作。RELU 是一个非线性激活函数。此操作应用于每个像素，并将特征图中的所有负像素值替换为零。卷积操作本身是线性的，为了建模非线性变化的图像特征，需要使用像RELU 这样的非线性函数向模型中引入非线性。

Q7：***对于给定的图像输入尺寸、Filter Size、  Stride 和 Padding大小，feature map 的尺寸是多少？***

- 如果我们的输入图像大小为 nxn，过滤器大小为 fxf，p 是padding size，s 是stride，则特征图的维度为：**floor[ ((n-f+2p)/s)+1] x floor[ ((n-f+2p)/s)+1]**

Q8：***解释Valid Paddding和Same Padding***

- valid padding：**当filter全部在image里面的时候，进行卷积运算**
- same padding：**当filter的中心(K)与image的边角重合时，开始做卷积运算；卷积之后输出的feature map尺寸保持不变**

Q9：****Pooling有哪些不同类型？说明他们的特点。****

- **max pooling：返回卷积核覆盖部分的最大值并抑制噪声**
- **average pooling：计算卷积核覆盖的特征图的平均值**
- sum pooling：计算卷积核窗口中所有元素的总和
- 最广泛使用的池化技术是**max pooling**，因为它捕获了最重要的特征。

Q12：**解释flatten层在CNN中的角色和作用**

- 在对图像的特征表示进行一系列卷积和池化操作之后，我们将最终池化层的输出展平为一个长的连续线性数组或向量。将所有生成的二维数组转换为向量的过程称为Flattening。
Flatten 输出作为输入提供给具有不同隐藏层数的完全连接的神经网络，以学习特征表示中存在的非线性复杂性。

Q13：****列出池化层的超参数****

- 池化层的超参数包括filter size、stride、max/average pooling，如果池化层的输入维度是$n_{h}*n_{w}*n_{c}$，那么输出维度将是$((n_{h}-f)/s+1)*((n_{w}-f)/s+1)*n_{c}$

Q14：****解释CNN中“参数共享”和“稀疏连接”的意义****

- 参数共享：在卷积中，我们在对输入进行卷积时共享参数。这背后的直觉是，对图像的一部分有用的特征检测器也可能对图像的另一部分有用。因此，通过使用单个卷积核，我们对所有整个输入进行了卷积，因此参数是共享的。
- 稀疏连接：对于每一层，每个输出值都取决于少量输入，而不是考虑所有输入。

Q15：**可以使用CNN执行降维操作吗？如果可以，CNN中哪个相关子层执行了降维操作？**

- CNN可以被用于降维操作
- 池化层，Pooling layer 的主要目标是减少 CNN 的空间维度。为了降低空间维度，它将执行下采样操作，通过在输入矩阵上滑动卷积核矩阵来创建池化特征图。
