# LLM

**Q1：讲解生成式语言模型的工作机理**

生成式语言模型将N个token作为输入，并一次生成一个token作为输出。然后它将该输出token合并为下一次迭代的输入的一部分，产生一个新的token输出，依此类推。此模式不断重复，直到达到停止条件，表明它已完成生成您需要的所有文本。

![https://miro.medium.com/v2/resize:fit:828/format:webp/1*zaRZlVk-dl0zVUOe0g_ufg.png](https://miro.medium.com/v2/resize:fit:828/format:webp/1*zaRZlVk-dl0zVUOe0g_ufg.png)

生成式语言模型的输出是所有可能标记的概率分布。概率分布向量中，每个值表示选择特定标记的概率。

![Untitled](LLM%204968d10b44c64639b1cecca08de3e8a5/Untitled.png)

这个概率分布来自训练阶段。在训练期间，模型会接触大量文本，并且在给定输入标记序列的情况下，调整其权重以预测良好的概率分布。

GPT 生成模型是通过大部分互联网进行训练的，因此它们的预测反映了它们所看到的信息的混合。

**Q2：解释LLM中token的概念**

LLM会将输入文本分解为多个片段，每一部分大约是一个单词大小的字符序列或更小的字符序列，这种字符序列就是token。

token可以是单词或只是字符块。例如，单词“hamburger”被分解为标记“ham”、“bur”和“ger”，而像“pear”这样的简短而常见的单词是单个标记。

**Q3：在NLP任务中，使用Transformer的架构与使用RNN的架构相比，有什么优势？**

- 在Transformer出现之前，通常使用LSTM与encoder-decoder架构完成序列到序列的任务；LSTM模型考虑到了单词之间的相互依赖，但是该模型有一个局限性：训练速度相对较慢，并且输入序列无法并行传递。Transformer的想法是在不使用循环网络的情况下维持序列中单词的相互依赖性，而仅使用处于其架构中心的注意力机制。注意力衡量两个序列的两个元素的相关程度。
- 在基于 Transformer 的架构中，使用self-attention。自注意力层确定同一序列中不同单词的相互依赖关系，以将相关表示与其相关联。以这句话为例：“狗没有过马路，因为它太累了”。对于人类来说，显然“它”指的是“狗”而不是“街道”。因此，自注意力过程的目标是检测“狗”和“它”之间的联系”。与RNN相比，self-attention可以并行训练，使得 Transformer 的训练速度更快，并且已被证明对噪声和丢失数据具有更强的鲁棒性。
- 另外，在上下文嵌入中，Transformer可以从上下文中提取信息来纠正丢失或嘈杂的数据，这是其他神经网络无法提供的。

**Q4：如何评估LLM的性能**

评估LLM有两种方法：外在评估和内在评估。

- **内在评估**
    
    捕获模型捕获它应该捕获的内容（例如概率）的程度。例如困惑度、交叉熵 ，困惑度是模型预测的单词的逆概率的几何平均值。困惑度越低，训练效果就越好。
    
- **外部评估（基于任务的评估）**
    
    捕获模型在特定任务中的有用程度。例如给出两个不同LLM的输出，令人类比较打分。
    

**Q5：LLM是如何预训练的？**

- **MLM，由BERT**等双向模型使用，其中训练集中一定比例的单词被屏蔽，模型的任务是预测这些缺失的单词。请注意，在此任务中，模型可以看到缺失单词之前和之后的单词，这就是它被称为双向的原因。
- **自回归**（例如 GPT），它们是单向的，预训练时看不到之后的单词，预测下一个单词。这是因为这些自回归模型是专门为更好的语言生成而设计的，这使得模型有必要以单向的方式进行预训练。

**Q6：讲讲NSP预训练任务**

- （NSP）用于语言建模，作为BERT模型训练过程的一半（另一半是掩码语言建模（MLM））。NSP的目标是预测一个句子是否逻辑上遵循模型呈现的另一个句子。
- 在训练过程中，模型会呈现成对的句子，其中一些在原始文本中是连续的，而另一些则不是。然后训练模型来预测给定的句子对是否相邻。这使得模型能够理解句子之间的长期依赖关系。

Q7：**LLM中的tokenisation技术**

- **tokenisation**是将原始文本转换为一系列较小单元（称为token）的过程，这些单元可以是单词、子词或字符。**LLM**中使用的一些**tokenisation**技术包括：
    - **基于单词的**：此方法将文本拆分为单独的单词，将*每个单词*视为单独的标记。虽然简单直观，但基于单词的标记化可能会难以处理词汇表之外的单词，并且可能无法有效地处理具有复杂形态的语言。
    - **基于子词的**：基于子词的方法，例如***字节对编码（BPE）***和***WordPiece***，将文本分割成*更小的单元*，这些单元可以组合成整个单词。这种方法使法学硕士能够处理词汇表之外的单词，并更好地捕捉不同语言的结构。例如，**BPE合并***最常*出现的字符对来创建子字单元，而 WordPiece 采用*数据驱动的*方法将单词分段为子字标记。
    - **基于字符的**：此方法将*单个字符*视为标记。尽管它可以处理任何输入文本，但基于字符的标记化通常需要更大的模型和更多的计算资源，因为它需要处理更长的标记序列。

Q8：在开发LLM时，怎样通过人为方法减少LLM的偏见？

- **训练数据管理**：人类可以参与管理和注释高质量和多样化的训练数据。这可能包括识别和纠正偏见、确保观点平衡以及减少有争议或冒犯性内容的影响。
- **模型微调**：专家可以通过提供有关模型输出的反馈来指导模型微调过程，通过RLHF帮助模型更好地避免有偏见或不正确的响应。
- **评估和反馈**：人工审核人员可以评估模型的性能并向开发人员提供反馈，然后开发人员可以迭代改进模型。
- **定制和控制**：可以为用户提供定制模型行为的选项，根据他们的喜好或要求调整输出。这种方法可以帮助用户通过根据特定上下文或领域定制模型来减轻模型响应中的偏差。

Q9：****哪些因素会导致LLM中的偏见？****

LLM偏见是指存在系统性的误述、归因错误或事实扭曲，导致偏向某些群体或想法，从而使刻板印象永久化，或根据学习的模式做出错误的假设。此类模型中的偏差可能由以下几个因素引起：

- **训练数据**：如果用于训练语言模型的数据包含来自源材料或通过选择过程的偏差，这些偏差可以被模型吸收并随后反映在其行为中。
- **算法**：偏差也可以通过用于处理和学习数据的算法引入。例如，如果算法更加重视某些特征或数据点，它可能会无意中引入或放大数据中存在的偏差

Q10：**在 Transformer 中，同一个单词在不同的句子中可以有不同的注意力权重吗？**

是的；Transformer架构查看整个句子和句子中单词的位置（位置嵌入），然后使用注意力权重对它们进行加权，计算出注意力权重。

引用：[https://stats.stackexchange.com/questions/575166/in-a-tranformer-the-same-word-can-have-different-attention-weights-in-different](https://stats.stackexchange.com/questions/575166/in-a-tranformer-the-same-word-can-have-different-attention-weights-in-different)

Q11：****LLM 中的因果语言建模CLM与屏蔽语言建模MLM有什么区别？****

- CLM 是一种自回归方法，其中模型经过训练以在给定先前标记的情况下预测序列中的下一个标记。CLM 用于 GPT-2 和 GPT-3 等模型，非常适合文本生成和摘要等任务。然而，CLM 模型具有单向上下文，这意味着它们在生成预测时仅考虑过去而不考虑未来上下文。
- MLM 是一种用于 BERT 等模型的训练方法，其中输入序列中的一些标记被屏蔽，模型学习根据周围的上下文来预测被屏蔽的标记。MLM 具有双向上下文的优势，允许模型在进行预测时考虑过去和未来的token。这种方法对于文本分类、情感分析和命名实体识别等任务特别有用。

引用：[https://medium.com/@tom_21755/understanding-causal-llms-masked-llm-s-and-seq2seq-a-guide-to-language-model-training-d4457bbd07fa](https://medium.com/@tom_21755/understanding-causal-llms-masked-llm-s-and-seq2seq-a-guide-to-language-model-training-d4457bbd07fa)

Q12：**RLHF完整训练过程是什么？**

- **步骤1：监督微调（SFT）** —— 使用精选的人类回答来微调预训练的语言模型以应对各种查询；
- **步骤2：奖励模型微调** —— 使用一个包含人类对同一查询的多个答案打分的数据集来训练一个独立的（通常比 SFT 小的）奖励模型（RW）；
- **步骤3：RLHF 训练** —— 利用 Proximal Policy Optimization（PPO）算法，根据 RW 模型的奖励反馈进一步微调 SFT 模型。

Q13：**GPT和bert有什么不一样？**

- GPT-3 是自回归模型，而 BERT 是双向的。GPT-3 在进行预测时仅考虑左侧上下文，而 BERT 会同时考虑左侧和右侧上下文。这使得 BERT 更适合情感分析或 NLU 等任务。
- GPT-3 是在 45TB 的数据上进行训练，而 BERT 是在 3TB 的数据上进行训练。因此，GPT-3 比 BERT 能够访问更多信息

Q14：**Wordpiece与BPE之间有什么区别？**

- WordPiece与BPE的最大区别在于，如何选择两个子词进行合并：BPE选择频数最高的相邻子词合并，而WordPiece选择能够提升语言模型概率最大的相邻子词加入词表。

Q15：****BERT 中的word embedding、position embedding和positional encoding有什么区别？****

word embedding是一种可学习的向量表示，每个词都被赋予一个one-hot编码，然后该编码充当索引，与该索引相对应的是一个维度向量，在训练模型时可以学习其中的系数。

position embedding类似于word embedding， 只不过它是用句子中的位置作为索引，而不是一种热编码。

positional encoding不是可学习的，而是选择的一种映射数学函数。

Q16：**Adaptive Softmax在LLM中有何用处？**

在NLP的大部分任务中，都会用到softmax,但是对于词汇量非常大的任务，每次进行完全的softmax会有非常大的计算量，很耗时(每次预测一个token都需要O(|V|)的时间复杂度)；

adaptive softmax提出利用单词分布不均衡的特点(unbalanced word distribution)来形成clusters, 这样在计算softmax时可以避免对词汇量大小的线性依赖关系，降低计算时间；

Q17：**如何减轻LLM中的“幻觉”现象**

LLM幻觉是指LLM生成的内容与给定的输入或源内容不一致，或者完全没有意义；LLM的幻觉现象是由于训练数据集有限、过时或矛盾，导致LLM在理解和回答用户问题时，往往依赖于自己学习到的统计规律或模式，而不是基于事实或逻辑；

减轻LLM幻觉的方法主要有以下几种：

- 人工审核：这种方法是在LLM生成内容后，由人工进行审核和修改，以确保内容的正确性和合理性。这种方法的优点是可以有效地避免或纠正幻觉，提高内容质量；缺点是需要耗费大量的人力和时间成本，而且可能存在人为的错误或偏见。
- 数据过滤：这种方法是在LLM生成内容前，对输入或源内容进行过滤和清洗，以去除不相关、不准确或不一致的信息。这种方法的优点是可以减少幻觉的可能性，提高内容的相关性和一致性；缺点是需要耗费大量的计算资源和算法技巧，而且可能存在数据丢失或过度简化的风险。
- 知识图谱：这种方法是在LLM生成内容时，利用一个结构化的知识库（知识图谱）来提供事实信息和逻辑推理，以增强LLM的理解和回答能力。这种方法的优点是可以提高内容的有意义性和忠实性，提供更丰富和更深入的信息；缺点是需要构建和维护一个大规模且高质量的知识图谱，而且可能存在知识图谱不完备或不更新的问题。

Q18：**有哪些方法可以降低LLM训练时的显存占用**

- 混合精度训练，例如AMP（Automatic Mixed Precision）。这种技术旨在在保持收敛性的同时最大化GPU张量核心的吞吐量。
- 梯度累积：梯度累积中，每批计算的量较小，并在多次迭代中累积梯度（通常求和或求平均），而不是在每个批次之后立刻更新模型权重。一旦累积的梯度达到目标「虚拟」批大小，模型权重就会用累积的梯度更新。
- QLora：考虑到LLM的参数的低秩属性（low intrinsic dimension），在做finetune的时候不做full-finetune，而是用一个降维矩阵A和一个升维矩阵B去做finetune。

![Untitled](LLM%204968d10b44c64639b1cecca08de3e8a5/Untitled%201.png)

Q19：**Lora微调方法的局限性**

基于低秩的微调可能并不always work，比如finetune与pretrain的gap过大的时候，比如中英差异；如LLama在Lora中文语料时效果不佳

Q20：**bf16，fp16半精度训练的区别**

bf16 用8bit 表示指数，7bit 表示小数；fp16用5bit 表示指数，10bit 表示小数。也就是说bf16 可表示的整数范围更广泛，但是精度较低；fp16 表示整数范围较小，但是精度较高。

尽管BF16的精度较低，但是它的表示范围较大，因此在深度学习中通常是更好的选择。此外，也是由于精度没有那么高，BF16在操作时需要的硬件资源也会较少。

Q21：**解释混合精度训练的具体流程**

同时有低精度和高精度的权重（float16和float32），前向传播用低精度来算，反向传播的gradients也用低精度来算，但是在更新参数的时候更新的是高精度的参数。然后在下一次的前向传播之前，对这个更新后的高精度参数量化为低精度参数，再开启下一次前向传播。

Q22：**RLHF过程中RM随着训练过程得分越来越高，效果就一定好吗？有没有极端情况？**

Q23：**解释P-tuning 的工作原理，并说明它与传统的 fine-tuning方法的不同之处。**